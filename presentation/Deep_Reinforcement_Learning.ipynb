{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "17_uTL2TSiEWQQnHKZS4TYX3lDcHnQbZ5",
      "authorship_tag": "ABX9TyMUbUN9vKGJNuTtqN5B+ZNc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernandofsilva/RL/blob/main/presentation/Deep_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyWtxNoV-JBq"
      },
      "source": [
        "# Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI2vbfJGPKYw"
      },
      "source": [
        "˜*Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.*\n",
        "\n",
        "-- <cite>Richard Sutton</cite>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl42rHUxRyej"
      },
      "source": [
        "## RL Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAFCFtEP_7-J"
      },
      "source": [
        "![image](https://media.springernature.com/lw1000/springer-static/image/chp%3A10.1007%2F978-981-15-4095-0_3/MediaObjects/492103_1_En_3_Fig1_HTML.png?as=jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6ZL2nL-KfA"
      },
      "source": [
        "## Basis\n",
        "- markov decision process\n",
        "- state value function\n",
        "- bellman equations\n",
        "- action value function\n",
        "- policies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cML3d3X_UYuf"
      },
      "source": [
        "![image](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ONPH9ZFe47"
      },
      "source": [
        "### Goals and Rewards\n",
        "\n",
        "In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the reward, passing from the environment to the agent. At each time step, the reward is a simple number, $R_t \\in \\mathbb{R}$. Informally, the agent’s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis:\n",
        "\n",
        "*That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSX8WNa_IAvP"
      },
      "source": [
        "### Returns and Episodes\n",
        "\n",
        "If the sequence of rewards received after time step t is denoted $R_{t+1}, R_{t+2}, R_{t+3}, . . .$, then what precise aspect of this sequence do we wish to maximize? In general, we seek to maximize the expected return, where the return, denoted Gt, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\n",
        "\n",
        "$Gt := R_{t+1} + R_{t+2} + R_{t+3} + ··· + R_T$ = $\\sum_{k=0}^{\\infty} R_{t+k+1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZtU0bpfK6wT"
      },
      "source": [
        "### Episodic or Continuing?\n",
        "\n",
        "A **task** is an instance of the reinforcement learning (RL) problem.\n",
        "\n",
        "**Continuing tasks** are tasks that continue forever, without end.\n",
        "\n",
        "**Episodic tasks** are tasks with a well-defined starting and ending point.\n",
        "\n",
        "In this case, we refer to a complete sequence of interaction, from start to finish, as an **episode**.\n",
        "Episodic tasks come to an end whenever the agent reaches a **terminal state**.\n",
        "\n",
        "$\\sum_{k=0}^{\\infty} \\lambda^{k} R_{t+k+1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2sE3zKGMqUN"
      },
      "source": [
        "## Markov Decision Process (MDP)\n",
        "\n",
        "Markov decision process problems (MDPs) assume a finite number of states and actions. At each time the agent observes a state and executes an action, which incurs intermediate costs to be minimized (or, in the inverse scenario, rewards to be maximized). The cost and the successor state depend only on the current state and the chosen action. Successor generation may be probabilistic, based on the uncertainty we have on the environment in which the search takes place.\n",
        "\n",
        "A (finite) Markov Decition Process is defined by:\n",
        "\n",
        "- a (finite) set os states $\\mathbf{S}$\n",
        "- a (finite) set of actions $\\mathbf{A}$\n",
        "- a set os Rewards $\\mathbf{R}$\n",
        "- the one step of dynamics of the environment\n",
        "- a discount rate $\\lambda \\in [0, 1]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lC_OofnSayK"
      },
      "source": [
        "## Policies and Value Functions\n",
        "\n",
        "The value function of a state s under a policy $\\pi$, denoted $v_\\pi(s)$, is the expected return when starting in $s$ and following $\\pi$ thereafter. For MDPs, we can define $v_\\pi$ formally by:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlnr7bFU-Knl"
      },
      "source": [
        "## Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VogFiFC-Kqz"
      },
      "source": [
        "## Temporal-Difference Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9lCjv5x-Kud"
      },
      "source": [
        "## Continuous Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "breH4wX3AypV"
      },
      "source": [
        "## Deep Q-Learning (DQN)"
      ]
    }
  ]
}